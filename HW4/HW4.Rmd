---
title: "Excersize 4"
author: "Colin Wick"
date: "5/4/2021"
output: html_document
---

```{r head,echo = F,message = F,warning = F}
knitr::opts_chunk$set(echo = F,message = F,warning = F)
```

```{r the-libs}
library(tidyverse)
library(LICORS)
library(igraph)
library(arules)
library(arulesViz)
```


# Question 1

```{r load-data-1}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv") %>%
  mutate(red = ifelse(color=="red",1,0))
```

```{r clustering-wine-color}
## Color clusters
wine_col <- wine %>% select(-color,-red)
wine_col_scl <- scale(wine_col,center = T,scale = T)

wine_col_hcl <- LICORS::kmeanspp(wine_col_scl,2)

merge(wine,wine_col_hcl$cluster,by=0) %>%
  rename("cluster" = y) %>% mutate(cluster=factor(cluster)) %>%
  count(cluster,color) %>% group_by(color) %>%
  mutate(prop = prop.table(n)) %>% select(-n) %>%
  pivot_wider(id_cols = cluster,names_from = color,values_from = prop) 
```

```{r clustering-wine-quality}
qual_levels <- levels(factor(wine$quality))  

wine_col <- wine %>% select(-color) %>% select(-quality) 

wine_col_scl <- scale(wine_col,center = T,scale = T)
wine_col_dist <- dist(wine_col_scl,method = "euclidean")

set.seed(42069)
wine_qual_hcl <- kmeanspp(wine_col_scl,k = 7,nstart = 50)

cormatrix <- merge(wine,wine_qual_hcl$cluster,by.x=0,by.y=0) %>% select(-Row.names,-color) %>% cor()
  ggcorrplot::ggcorrplot(corr = cormatrix)
  
merge(wine,wine_qual_hcl$cluster,by.x=0,by.y=0) %>%
  rename("cluster" = y) %>% mutate(cluster=factor(cluster)) %>%
  ggplot()+
  geom_jitter(aes(x=cluster,y=quality,color=color),alpha=.3)
```


```{r PCA-color}
wine_col <- wine %>% select(-color) %>% select(-red)
wine_col_scl <- scale(wine_col,center = T,scale = T)

PCAwine = prcomp(wine_col, scale=TRUE, rank=2)
summary(PCAwine)

PCAwine_summary <- PCAwine$rotation %>%
  data.frame() %>%
  rownames_to_column("var")

merge(wine, PCAwine$x[,1:2], by="row.names") %>%
  ggplot()+
  geom_violin(aes(x=factor(color),y=PC1))

cormatrix <- merge(wine, PCAwine$x[,1:2], by=0) %>% select(-Row.names,-color) %>% cor()
  ggcorrplot::ggcorrplot(corr = cormatrix)
```

```{r PCA-quality}
wine_col <- wine %>% select(-color) %>% select(-quality)
wine_col_scl <- scale(wine_col,center = T,scale = T)

PCAwine = prcomp(wine_col, scale=TRUE, rank=7)
summary(PCAwine)

PCAwine_summary <- PCAwine$rotation %>%
  data.frame() %>%
  rownames_to_column("var")

merge(wine, PCAwine$x[,1:3], by="row.names") %>%
  ggplot()+
  geom_violin(aes(x=factor(quality),y=PC1,fill="PC1"),alpha=.3)+
  geom_violin(aes(x=factor(quality),y=PC2,fill="PC2"),alpha=.3)+
  geom_violin(aes(x=factor(quality),y=PC3,fill="PC3"),alpha=.3)

wine_pca <- merge(wine, PCAwine$x[,1:7], by=0)

lm1 <- lm(data=wine_pca,quality ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7)
plot(data=wine_pca,quality ~ lm1$fitted.values)

summary(lm1)
```

# Question 2

```{r}
sm <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv")

sm %>% select(-X,-chatter) %>%
  colSums() %>% data.frame() %>% arrange(desc(`.`)) %>% head(10) %>%
  rownames_to_column() %>%
  rename("Topic" = 1,"Count"=2) %>%
  kableExtra::kbl() %>%
  kableExtra::kable_minimal()

top10 <- sm %>% select(-X,-chatter) %>%
  colSums() %>% data.frame() %>% arrange(desc(`.`)) %>% head(10) %>% rownames()
```

From here we see the beginnings of the market structure just by looking at aggregate mentions of each topic, but this is not a detailed marketing strategy. 

```{r include=FALSE}
# This is a good song
# https://open.spotify.com/track/2ArusOBCO5xbJV6QlreriJ?si=4512886a9b3c4773

sm_net <- sm %>%
  pivot_longer(-X) %>%
  mutate(edge = ifelse(value != 0,1,0)) %>%
  filter(edge != 0 & value > 5)%>% unique() %>% filter(name != "chatter") %>%
  select(name,X) %>% data.frame()

sm_net_list = split(x = sm_net$name,f=sm_net$X) 
sm_net_list <- as(sm_net_list,"transactions")

topic_assoc = apriori(sm_net_list, 
	parameter=list(support=.01, confidence=.5, maxlen=2))
```

```{r}
plot(head(topic_assoc, 25, by='lift'), method="graph" )
```

We removed the "chatter" category because everyone chatters now and then. Instead, we look at the likelihood of people to talk about a topic based on talking about another. The visualization above creates groups based on people's likely topics of conversation. Though most of these seem intuitive, the clear topic groupings. 

The size of each node represents the amount of times it appeared in the data, meaning a higher share of the audience consistently talked about the topic. From this we see multiple clear segments; 

1. An outdoorsy, personal health and fitness type consumer, likely to be marketed to via signage and retail placement.

2. An online gaming college student consumer, likely to be marketed to on Twitch and other gaming-oriented social media sites.

3. A beauty, cooking, fashion, and photo sharing consumer, likely marketed to on Instagram.

4. Politics, news, travel, and automotive interested consumer, likely marketed to on Facebook and Twitter.

5. The food, religion, parenting consumer, who may be more difficult to reach in a measured way due to the low relative concentration on any particular digital media. Grocery stores and traditional media would be likely places to build your brand.

```{r}
sm1 <- sm %>%
  column_to_rownames("X")
sm_scl <- sm1 %>% scale(center=T,scale = T) 

  mu = attr(sm_scl,"scaled:center")
  sigma = attr(sm_scl,"scaled:scale")


clust1 <- kmeanspp(sm_scl,k = 5)
sm1$Total = rowSums(sm1)
sm1$cluster <- clust1$cluster
```

```{r}
clusters <- data.frame(matrix(nrow=5,ncol=length(names(sm1))))
clusters[1,] <- colSums(sm1[sm1$cluster==1,])
clusters[2,] <- colSums(sm1[sm1$cluster==2,])
clusters[3,] <- colSums(sm1[sm1$cluster==3,])
clusters[4,] <- colSums(sm1[sm1$cluster==4,])
clusters[5,] <- colSums(sm1[sm1$cluster==5,])
names(clusters) <- names(sm1)

cluster_vars <- data.frame(matrix(nrow=5,ncol=5))

cluster_vars[,1] <- clusters %>% select(-Total,-chatter) %>% mutate(cluster = c(1:5)) %>% t() %>% data.frame() %>% arrange(desc(X1)) %>% head(5) %>% row.names()
cluster_vars[,2] <- clusters %>% select(-Total,-chatter) %>% mutate(cluster = c(1:5)) %>% t() %>% data.frame() %>% arrange(desc(X2)) %>% head(5) %>% row.names()
cluster_vars[,3] <- clusters %>% select(-Total,-chatter) %>% mutate(cluster = c(1:5)) %>% t() %>% data.frame() %>% arrange(desc(X3)) %>% head(5) %>% row.names()
cluster_vars[,4] <- clusters %>% select(-Total,-chatter) %>% mutate(cluster = c(1:5)) %>% t() %>% data.frame() %>% arrange(desc(X4)) %>% head(5) %>% row.names()
cluster_vars[,5] <- clusters %>% select(-Total,-chatter) %>% mutate(cluster = c(1:5)) %>% t() %>% data.frame() %>% arrange(desc(X5)) %>% head(5) %>% row.names()
names(cluster_vars) <- c("Cluster 1","Cluster 2","Cluster 3","Cluster 4","Cluster 5")

cluster_vars %>% kableExtra::kbl() %>% kableExtra::kable_minimal()
```

Above is the top topics that emerged per-cluster after a completely different algorithm was run against the data. Notice the same pattern emerges under this methodology, emphasizing the natural audiences in the data. 

# Question 3 - Market Segmentation

First we load the data.

```{r}
data("Groceries")
```

We can examine the sparse matrix of transactions and items in a simple plot.

```{r}
image(sample(Groceries, 100))
dev.off() 
```

What are the most frequent item purchases?

```{r}
frequent_items <- eclat(Groceries, parameter = list(supp = 0.07, maxlen = 15)) 
summary(frequent_items)
```

Next, we use the `apriori` function, which implements the Apriori algorithm to mine frequent itemsets, to define rules for purchasing associations.  

```{r}
rules <- apriori(Groceries, parameter = list(supp = 0.001, conf = 0.8, maxlen = 3)) # Min Support as 0.001, confidence as 0.8.
# remove redundant rules (not needed)
# subset_matrix <- is.subset(rules, rules)
# subset_matrix[lower.tri(subset_matrix, diag = T)] <- NA # not working
# redundant <- colSums(subset_matrix, na.rm = T) >= 1
# rules_pruned <- rules[!redundant]
# rules <- rules_pruned
summary(rules)
```

Some of the rules can be visualized.

```{r}
plot(rules, engine = "ggplot") + theme_clean()
plot(rules, "scatterplot", engine = "ggplot") + theme_clean()
plot(rules, "grouped", engine = "default") 
head(quality(rules)) %>% kbl(digits = 4, "pipe")
arules::itemFrequencyPlot(
  Groceries,
  topN = 15,
  col = 'dodgerblue',
  main = 'Relative Item Frequency Plot',
  type = "relative",
  ylab = "Item Frequency"
  )
plot(rules, method = "graph", control = list(type = "items"), engine = "igraph")
plot(rules, method = "paracoord", control = list(type = "items"))
```

From the visualizations and the summaries of item pairs, I recommend the following aisles:

1. Groceries Aisle – Milk, Eggs and Vegetables
2. Liquor Aisle – Liquor, Red/Blush Wine, Bottled Beer, Soda
3. Eateries Aisle – Herbs, Tropical Fruits, Rolls/Buns, Fruit Juices, Jams
4. Breakfast Aisle – Cereals, Yogurt, Rice, Curd

# Question 4 - Author Attribution

First, we load the data.

```{r}
## Collect data
# training data
Data_train <- readtext(Sys.glob('../../ECO395M/data/ReutersC50/C50train/*'))
# head(Data_train$text, n = 1)
# testing data
Data_test <- readtext(Sys.glob('../../ECO395M/data/ReutersC50/C50test/*'))
```

Then we pull author names from the file directory and assign them to texts, and do a check to make sure it worked as expected.

```{r}
# author names
author_names <- as.data.frame(rep(basename(list.dirs('../../ECO395M/data/ReutersC50/C50train')), each = 50))
author_names <- author_names[-(1:50),]
# assign author name to Text
Data_test$author <- author_names
Data_train$author <- author_names
# dropping ID column
Data_test <- Data_test[-1]
Data_train <- Data_train[-1]
# converting author column to factor
Data_test$author <- as.factor(Data_test$author)
Data_train$author <- as.factor(Data_train$author)
# did it work?
table(Data_train$author) %>% kbl("pipe")
```

Next, we create the corpus. This is split into a train/test and are stripped of punctuation, forced to lowercase, and numbers are removed --- as well as whitespace and common stopwords. I use simple wordclouds to check if this process is working as expected.

```{r}
## Explore and Prep
# Create corpus
test_corpus <- Corpus(VectorSource(Data_test$text))
train_corpus <- Corpus(VectorSource(Data_train$text))
# clean corpus
test_corpus <-
  test_corpus %>%
  tm_map(., content_transformer(tolower)) %>%
  tm_map(., content_transformer(removeNumbers)) %>%
  tm_map(., content_transformer(removePunctuation)) %>%
  tm_map(., content_transformer(stripWhitespace)) %>%
  tm_map(., content_transformer(removeWords), stopwords("SMART"))
# did it work?
# inspect(test_corpus[1])
wordcloud(test_corpus, min.freq = 40, random.order = FALSE)
train_corpus <-
  train_corpus %>%
  tm_map(., content_transformer(tolower)) %>%
  tm_map(., content_transformer(removeNumbers)) %>%
  tm_map(., content_transformer(removePunctuation)) %>%
  tm_map(., content_transformer(stripWhitespace)) %>%
  tm_map(., content_transformer(removeWords), stopwords("SMART"))
```

To analyze the text, I create document-term matrices from the corpuses.

```{r}
# document term matrix (sparse matrices)
test_dtm <- DocumentTermMatrix(test_corpus)
train_dtm <- DocumentTermMatrix(train_corpus)
# inspect(train_dtm)
```

Finally, with the document-term matrices, I use a naive-bayes classifier to predict the author of the text using a dictionary of words unique to each article.

```{r}
## Naive Bayes Classification
freq_words <- findFreqTerms(train_dtm, 5)
# saving List using Dictionary() Function
Dictionary <- function(x) {
  if (is.character(x)) {
    return(x)
  }
  stop('x is not a character vector')
}
data_dict <- Dictionary(findFreqTerms(train_dtm, 5))
# appending Document Term Matrix to Train and Test Dataset 
data_train <- DocumentTermMatrix(train_corpus, list(data_dict))
data_test <- DocumentTermMatrix(test_corpus, list(data_dict))
# converting the frequency of word to count
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0)
  x <- factor(x, levels = c(0, 1), labels = c("No", "Yes")) 
  return(x)
}
# appending count function to Train and Test Dataset
data_train <- apply(data_train, MARGIN = 2, convert_counts)
data_test <- apply(data_test, MARGIN = 2, convert_counts)
# train model
data_classifier <- naiveBayes(data_train, Data_train$author)
data_test_pred <- predict(data_classifier, data_test)
# CrossTable(data_test_pred, Data_test$author,
#            prop.chisq = FALSE, prop.t = FALSE,
#            dnn = c('predicted', 'actual'))
```

I apply the trained model to the test set and compare the "actual author" to the predicted author. 

```{r}
final_df <- 
  tibble(
    "predicted" = data_test_pred,
    "actual" = Data_test$author
  )
num_correct <- 
  final_df %>% 
  mutate(correct = if_else(predicted == actual, 1, 0)) %>%
  pull(correct) %>%
  sum()
num_rows <- final_df %>% nrow()
num_correct / num_rows
```

This model guesses correctly (out of 50 authors) 70\% of the time.


